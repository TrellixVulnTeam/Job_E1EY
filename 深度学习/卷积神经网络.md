[参考链接](https://baike.baidu.com/item/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/17541100)
- 卷积神经网络的发展
    - 第一个卷积神经网络 1987-Alexander Waiber等提出的时间延迟网络
    - Yan LeCun LeNet
- 卷积神经网络的结构
    - 卷积层
        - 局部连接 
        - 参数共享
        - 三种卷积方式 Vaild(输出尺寸减小) Same(输出尺寸不变) Full(输出尺寸变大) 
    - 池化层
    - 全连接层 √
    - 激活函数(提高网络的表达能力与拟合能力 表达复杂的特征)
        - sigmoid
        - relu
        - tanh
        - leakyrelu
    - 池化层的反向传播 √

        Relu函数在数学上的定义为连续不可微的函数，它的定义为 ：当x>0时，Relu(x) = x,   当x<=0时，Relu(x) = 0。Relu函数在x=0处是不可微的，但是在深度学习框架的代码中为了解决这个直接将其在x=0处的导数置为1，所以它的导数也就变为了，即：当x>0时，Relu'(x) = 1, 当x<=0时，Relu'(x) = 0。也就是强行可导了,而且用代码实现起来也非常简单。加个if判断就解决了。

        卷积神经网络中另外一个不可导的部分就是池化层，因为池化操作使得特征图的尺寸变化，假如做2×2的池化，假设那么第l+1层的特征图有16个梯度，那么第l层就会有64个梯度，这会使梯度无法对位的进行反向传播。       解决这个问题的思路也很简单。把1个像素的梯度传递给4个像素，但是需要保证传递的梯度总和不变。根据这条原则，平均池化层和最大池化层的反向传播也是一样的。平均池化层的前向传播就是把一个patch中的值求取平均来做池化，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度之和保持不变。平均池化层不能简单的吧梯度复制n遍直接反向传播回去，这样会使得loss之和变为原来的n倍，这样网络会梯度爆炸。

        最大池化也必须满足梯度之和不变的原则，最大池化的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0。最大池化与平均池化前向传播有一个不同点在于最大池化时需要记录下池化操作时到底哪个像素的值是最大。

    - [梯度消失问题与梯度爆炸问题](https://zhuanlan.zhihu.com/p/72589432#:~:text=%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%E5%92%8C%E6%A2%AF,%E4%B8%8A%E5%85%B6%E5%AE%9E%E6%98%AF%E4%B8%80%E6%A0%B7%E7%9A%84%E3%80%82)

        目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。

        而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。梯度消失或梯度爆炸在本质原理上其实是一样的。

        【梯度消失】经常出现，产生的原因有：一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。

        【梯度爆炸】一般出现在深层网络和权值初始化值太大的情况下。在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。

        梯度爆炸会伴随一些细微的信号，如：①模型不稳定，导致更新过程中的损失出现显著变化；②训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。

        [解决方法] 预训练+微调/梯度剪切/权重正则化/选择relu等梯度大部分落在常数上的激活函数/BN就是通过对每一层的输出规范为均值和方差一致的方法，消除了权重参数放大缩小带来的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。/相比较于以前直来直去的网络结构，残差中有很多这样（如上图所示）的跨层连接结构，这样的结构在反向传播中具有很大的好处，可以避免梯度消失。









