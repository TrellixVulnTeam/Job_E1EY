# 常见的损失函数
-   回归问题
    -   均方误差损失函数MSE
    -   平均绝对值误差MAE 对异常值有更好的鲁棒性
-   分类问题
    -   对数损失函数
    -   指数损失函数
    -   交叉熵损失函数CE

# CNN流行的原因
- 局部连接 
- 权值共享 减小参数量
- 池化操作 增大感受野
- 多层次结构 可以提取low-level和high-level的信息

# CNN不适用的场景
- 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

- 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

# 当神经网络效果不好时，应该从哪些方面考虑解决该问题？
- 是否找到合适的损失函数？（不同问题适合不同的损失函数）（理解不同损失函数的适用场景）

- batch size是否合适？batch size太大 -> loss很快平稳，batch size太小 -> loss会震荡（理解mini-batch）

- 是否选择了合适的激活函数？（各个激活函数的来源和差异）

- 学习率，学习率小收敛慢，学习率大loss震荡（怎么选取合适的学习率）

- 是否选择了合适的优化算法？（比如adam）（理解不同优化算法的适用场景）

- 是否过拟合？(深度学习拟合能力强，容易过拟合)（理解过拟合的各个解决方案）
    - Early Stopping
    - Regularization（正则化）
    - Weight Decay（收缩权重）
    - Dropout（随机失活）
    - 调整网络结构
# 深度学习中加速收敛/降低训练难度的方法有哪些？
- 瓶颈结构、残差、学习率、步长、动量、优化方法、预训练。

# K-means算法流程？它与KNN的区别？
-   a. 初始化k个聚类中心c1,c2,...,ck；
-   b. 对于每个样本xi和 每个聚类中心cj，计算样本与聚类中心之间的距离dij；
-   c. 对于每个样本xi，基于其最小的dij把其分配到第j个类Cj；
-   d. 对于每个类Cj，计算其所有样本均值作为新的聚类中心，重复步骤2和步骤3直至样本点所属的类不再变化或达到最大迭代次数；

# 常见的优化参数更新方法


# 几个著名的网络
-   Lenet 3+2
-   alexnet 5+3 
    - 使用7x7 5x5的卷积核
    - relu dropout 
-   vggnet
    - 使用3x3卷积核 1x1卷积 代替大的卷积核
-   google net

-   resnet