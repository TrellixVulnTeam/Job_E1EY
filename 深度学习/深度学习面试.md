# BatchNormalization的作用
-   神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。
-   BatchNormalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。

# 梯度消失
-   在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了。这种现象叫做消失的梯度问题。

# 循环神经网络，为什么好?
-    循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈连接又有前馈连接，这使得RNN可以更加容易处理不分段的文本等。

# 什么是Group Convolution
若卷积神将网络的上一层有N个卷积核,则对应的通道数也为N。设群数目为M,在进行卷积操作的时候,将通道分成M份,每个group对应N/M个通道,然后每个group卷积完成后输出叠在一起,作为当前层的输出通道。

# 什么是RNN
一个序列当前的输出与前面的输出也有关,在RNN网络结构中中,隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出,网络会对之前的信息进行记忆并应用于当前的输入计算中。

# 训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?
并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。

# 图像处理中锐化和平滑的操作
- 锐化就是通过增强高频分量来减少图像中的模糊,在增强图像边缘的同时也增加了图像的噪声。平滑与锐化相反,过滤掉高频分量,减少图像的噪声使得图片变得模糊。


# VGG使用3*3卷积核的优势是什么?
-   2个3*3的卷积核串联和5*5的卷积核有相同的感知野,前者拥有更少的参数。多个3*3的卷积核比一个较大尺寸的卷积核有更多层的非线性函数,增加了非线性表达,使判决函数更具有判决性。

# Relu比Sigmoid的效果好在哪里?
-   Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而relu在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。

# 神经网络中权重共享的是？
-   卷积神经网络、循环神经网络

# 神经网络激活函数？
-   [sigmod、tanh、relu](https://www.jianshu.com/p/857d5859d2cc)

# 在深度学习中，通常会finetuning已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？
- 实践中的数据集质量参差不齐，可以使用训练好的网络来进行提取特征。把训练好的网络当做特征提取器。

# 什么是dropout
-   在神经网络的训练过程中,对于神经单元按一定的概率将其随机从网络中丢弃,从而达到对于每个mini-batch都是在训练不同网络的效果,防止过拟合。

# DropConnect的原理
- 防止过拟合方法的一种,与dropout不同的是,它不是按概率将隐藏层的节点输出清0,而是对每个节点与之相连的输入权值以一定的概率清0。
  
# BN层的作用，为什么要在后面加伽马和贝塔，不加可以吗
-   BN层的作用是把一个batch内的所有数据，从不规范的分布拉到正态分布。这样做的好处是使得数据能够分布在激活函数的敏感区域，敏感区域即为梯度较大的区域，因此在反向传播的时候能够较快反馈误差传播。
- 加入缩放平移变量γ和β的原因是： 不一定每次都是标准正态分布，也许需要偏移或者拉伸。直白的说就是为了保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。 这两个参数是用来学习的参数。

# GAN网络的思想
-   GAN用一个生成模型和一个判别模型,判别模型用于判断给定的图片是不是真实的图片,生成模型自己生成一张图片和想要的图片很像,开始时两个模型都没有训练,然后两个模型一起进行对抗训练,生成模型产生图片去欺骗判别模型,判别模型去判别真假,最终两个模型在训练过程中,能力越来越强最终达到稳态。

# 1*1的卷积作用
-   实现跨通道的交互和信息整合,实现卷积核通道数的降维和升维,可以实现多个feature map的线性组合,而且可是实现与全连接层的等价效果。

# 1*1的卷积作用
-   实现跨通道的交互和信息整合,实现卷积核通道数的降维和升维,可以实现多个feature map的线性组合,而且可是实现与全连接层的等价效果。

# 怎么提升网络的泛化能力
-   从数据上提升性能:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。
从算法调优上提升性能:用可靠的模型诊断工具对模型进行诊断,权重的初始化,用小的随机数初始化权重。对学习率进行调节,尝试选择合适的激活函数,调整网络的拓扑结构,调节batch和epoch的大小,添加正则化的方法,尝试使用其它的优化方法,使用early stopping。


# 怎么提升网络的泛化能力
-   从数据上提升性能:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。
从算法调优上提升性能:用可靠的模型诊断工具对模型进行诊断,权重的初始化,用小的随机数初始化权重。对学习率进行调节,尝试选择合适的激活函数,调整网络的拓扑结构,调节batch和epoch的大小,添加正则化的方法,尝试使用其它的优化方法,使用early stopping。

# 激活函数的作用
-   激活函数是用来加入非线性因素的,提高神经网络对模型的表达能力,解决线性模型所不能解决的问题。

# 为什么用relu就不用sigmoid了
-   Sigmoid的导数只有在0的附近时有比较好的激活性,在正负饱和区域的梯度都接近0，会导致梯度弥散。而relu函数在大于0的部分梯度为常数,不会产生梯度弥散现象。Relu函数在负半区导数为0,也就是说这个神经元不会经历训练,就是所谓稀疏性。而且relu函数的导数计算的更快。

# 防止过拟合有哪些方法
-   1）Dropout ；2）加L1/L2正则化；3）BatchNormalization ；4）网络bagging

# dropout咋回事讲讲
-   Dropout的目标是在指数 级数量的神经网络上近似这个过程。Dropout训练与Bagging训练不太一样。在Bagging的情况下,所有模型是独立的。
在Dropout的情况下,模型是共享参数的,其中每个模型继承的父神经网络参 数的不同子集。参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在Bagging的情况下,每一个模型在其相应训练集上训练到收敛。
-   在Dropout的情况下,通常大部分模型都没有显式地被训练,通常该模型很大,以致到宇宙毁灭都不 能采样所有可能的子网络。取而代之的是,可能的子网络的一小部分训练单个步骤,参数共享导致剩余的子网络能有好的参数设定。


# 神经网络为啥用交叉熵。
-   通过神经网络解决多分类问题时，最常用的一种方式就是在最后一层设置n个输出节点，无论在浅层神经网络还是在CNN中都是如此，比如，在AlexNet中最后的输出层有1000个节点，而即便是ResNet取消了全连接层，也会在最后有一个1000个节点的输出层。
-   一般情况下，最后一个输出层的节点个数与分类任务的目标数相等。假设最后的节点数为N，那么对于每一个样例，神经网络可以得到一个N维的数组作为输出结果，数组中每一个维度会对应一个类别。在最理想的情况下，如果一个样本属于k，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即[0,0,1,0,….0,0]，这个数组也就是样本的Label，是神经网络最期望的输出结果，交叉熵就是用来判定实际的输出与期望的输出的接近程度。